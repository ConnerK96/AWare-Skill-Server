# coding=UTF-8
#!/usr/bin/env python3
from __future__ import unicode_literals, print_function

from flask import Flask
from flask import Flask, flash, render_template, request
import urllib.parse
import json
import os.path
import os
import requests
import urllib.request
import urllib
import random
import shutil
import string
import re
import uuid
import importlib
from glob import glob
from pathlib import Path
from whatthelang import WhatTheLang
from googletrans import Translator
from fallback import fallbackHandler, google
from snips_nlu import SnipsNLUEngine
from snips_nlu.default_configs import CONFIG_EN
import linecache
import sys
from searxapi import callAPI
import json
from simpletransformers.classification import MultiLabelClassificationModel
import pandas as pd
import logging
from glob import glob
import argparse
from transformers import MarianSentencePieceTokenizer, MarianMTModel
from typing import List


# define por to listen
ap = argparse.ArgumentParser()
ap.add_argument("-p", "--port", required=False, type=int, default = 5000, help="Number of the port")
args = vars(ap.parse_args())

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

# save loaded modells in ram for more speed
snips_engines = {"firstkey": "firstkey"}

transformer_engines = {"firstkey": "firstkey"}

transformer_keys = {"firstkey": "firstkey"}

translation_engines = {"firstkey": "firstkey"}

translation_tokenizers = {"firstkey": "firstkey"}

list = glob("models/*.keys")
for x in list:
	try:
		lang = x.split("/")[1]
		lang = lang.split(".")[0]
		myfile = open("models/" + lang + ".keys","r")
		keys = myfile.read().split("\n")
		myfile.close()
		numKeys = len(keys)-1
		transformer_keys[lang] = keys

		model = MultiLabelClassificationModel('roberta',"models/" +  lang + '_transformer', num_labels=numKeys, use_cuda = False, args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 15, "train_batch_size": 16, "eval_batch_size": 16, 'no_cache': True, 'use_cached_eval_features' : False, 'save_model_every_epoch':False})
		transformer_engines[lang] = model

		snips_engines[lang] = SnipsNLUEngine.from_path("models/" + str(lang))

		print("**loaded " + lang + " ***")

	except Exception as e:
		print(e)



translationList = ["en","it","es","da","fr"]
for lgc in translationList:
	# from de to language
	mname = 'Helsinki-NLP/opus-mt-de-' + lgc
	model = MarianMTModel.from_pretrained(mname)
	tok = MarianSentencePieceTokenizer.from_pretrained(mname)

	translation_engines[mname] = model
	translation_tokenizers[mname] = tok

	# form language to de
	mname = 'Helsinki-NLP/opus-mt-' + lgc + "-de"
	model = MarianMTModel.from_pretrained(mname)
	tok = MarianSentencePieceTokenizer.from_pretrained(mname)

	translation_engines[mname] = model
	translation_tokenizers[mname] = tok

	print("**loaded translator: " + lgc + " ***")


# detect language using the wtl api
def detect(text):
	wtl = WhatTheLang()
	return wtl.predict_lang(text)


# info function for a better view
def info(text):
	print("\n")
	print("*"*5)
	print(text)
	print("*"*5)
	print("\n")

# translate string to other language
def translate(text, target = "en", src = False):
	global translation_engines
	global translation_tokenizers
	wtl = WhatTheLang()
	translated = None
	langcode = wtl.predict_lang(text)
	if(langcode != target):
		try:
			mname = f'Helsinki-NLP/opus-mt-{langcode}-{target}'   # model name
			model = None
			tok = None

			if(mname in translation_engines.keys()):
				model = translation_engines[mname]
				tok = translation_tokenizers[mname]
			else:
				model = MarianMTModel.from_pretrained(mname)
				tok = MarianSentencePieceTokenizer.from_pretrained(mname)

				translation_engines[mname] = model
				translation_tokenizers[mname] = tok


			batch = tok.prepare_translation_batch(src_texts=[text])  # don't need tgt_text for inference
			gen = model.generate(**batch)  # for forward pass: model(**batch)
			words: List[str] = tok.decode_batch(gen, skip_special_tokens=True)
			text = words[0]

			if(src):
				return text,langcode
			


		except:
			info("translating...")
			translator = Translator()
			translated = translator.translate(text, dest = target)
			text = translated.text
			if(src):
				return text,translated.src
	return text



# define class for nlu handling
class serverHelpers:
	def __init__(self, data):
		self.data = data
		self.text = data["text"]
		self.translated = None
		self.nlu_parsing = None
		self.probability = 0.6
		self.answer_type = "answer"
		self.useTranslator = True
		self.mainlang = "de"

		if("lang" in data):
			self.lang = data["lang"]
		else:
			self.detect()

	# info function for a better view
	def info(self,text):
		print("\n")
		print("*"*5)
		print(text)
		print("*"*5)
		print("\n")

	# detect language using the wtl api
	def detect(self):
		wtl = WhatTheLang()
		self.lang = wtl.predict_lang(self.text)

	# translation handling
	def translate(self,text = None, target = "de", src = False):
		global translation_engines
		global translation_tokenizers
		if(text == None):
			text = self.text
		wtl = WhatTheLang()
		try:
			langcode = wtl.predict_lang(text)
			if(langcode != target):
				try:
					mname = f'Helsinki-NLP/opus-mt-{langcode}-{target}'   # model name
					model = None
					tok = None

					if(mname in translation_engines.keys()):
						model = translation_engines[mname]
						tok = translation_tokenizers[mname]
					else:
						model = MarianMTModel.from_pretrained(mname)
						tok = MarianSentencePieceTokenizer.from_pretrained(mname)

						translation_engines[mname] = model
						translation_tokenizers[mname] = tok


					batch = tok.prepare_translation_batch(src_texts=[text])  # don't need tgt_text for inference
					gen = model.generate(**batch)  # for forward pass: model(**batch)
					words: List[str] = tok.decode_batch(gen, skip_special_tokens=True)
					self.translated = words[0]

					if(src):
						self.lang = langcode

					self.info("translated local...")




				except Exception as e:
					print(e)					
					self.info("translating...")
					translator = Translator()
					translated = translator.translate(text, dest = target)
					self.translated = translated.text
					if(src):
						self.lang = translated.src

			else:
				self.translated = text
		except:
			self.translated = text

	# format answer text
	def parseAnswer(self, text):
		text = str(text)
		text = re.sub("[\(\[].*?[\)\]]", "", text)
		text = text.replace("(","")
		text = text.replace(")","")
		return text


	"""
	run fallback logic
	if fallback has no answer too, return i am sorry
	"""
	def callFallback(self):
		#self.translate(target="en")
		try:
			#answer = fallbackHandler(self.translated,json.dumps(self.data, indent=2, ensure_ascii=False))
			answer = False
			if(answer != False and answer != "False"):
				self.nlu_parsing["skill_category"] = "fallback"
				answer = str(answer).encode().decode()
				print(answer)
				self.translate(text = answer,target = self.lang)
				answer = self.translated
				self.nlu_parsing["speak"] = self.parseAnswer(answer)
				if(len(self.nlu_parsing["speak"]) < 1):
					return "An error occured"
				self.nlu_parsing["answer_url"] = "ask_search_engine"
				self.nlu_parsing["answer_type"] = self.answer_type
				results = json.dumps(self.nlu_parsing, indent=2, ensure_ascii=False)
				self.info("Fallback succesfull")
				return results
			else:
				self.info("Fallback found no answer")
				self.nlu_parsing["skill_category"] = "unknown"
				lang = self.nlu_parsing["lang"]
				self.nlu_parsing["answer_type"] = self.answer_type
				self.nlu_parsing["answer_url"] = "ask_search_engine"

				de = ["Tut mir leid. Das weiÃŸ ich noch nicht, aber vielleicht hilft dir diese Website"]
				en = ["Sorry, I dont know this at the moment but maybe this webpage can help you"]

				if(lang == "de"):
					self.nlu_parsing["speak"] = random.choice(de)
				else:
					self.nlu_parsing["speak"] = random.choice(en)

				results = json.dumps(self.nlu_parsing, indent=2, ensure_ascii=False)
				return results


		except Exception as e:
			print(e)
			return "An error occured"


	# function to use transformers for intent classification
	def nn(self, langu, text):
		global transformer_keys
		global transformer_engines
		try:

			if(langu in transformer_keys.keys()):
				keys = transformer_keys[langu]
				numKeys = len(keys)-1
			else:
				myfile = open("models/" + langu + ".keys","r")
				keys = myfile.read().split("\n")
				myfile.close()
				numKeys = len(keys)-1
				transformer_keys[langu] = keys


			if(langu in transformer_engines.keys()):
				model = transformer_engines[langu]
			else:
				model = MultiLabelClassificationModel('roberta',"models/" +  langu + '_transformer', num_labels=numKeys, use_cuda = False, args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 15, "train_batch_size": 16, "eval_batch_size": 16, 'no_cache': True, 'use_cached_eval_features' : False, 'save_model_every_epoch':False})
				transformer_engines[langu] = model

			predictions, raw_outputs = model.predict([text])
			for x in range(numKeys):
				if(predictions[0][x] == 1):
					if(str(self.nlu_parsing["intent"]["intentName"]) == str(keys[x])):
						print("Transformer: " + keys[x] + ": " + str(raw_outputs[0][x]))
						return True

			return False

		except Exception as e:
			print(e)
			return None



	# first nlu run for original language
	def nlu(self):
		global snips_engines
		# try to load nlu for given language
		try:
			if(self.lang in snips_engines.keys()):
				nlu_engine = snips_engines[self.lang]
			else:
				print("*** " + self.lang + " ***")
				snips_engines[self.lang] = SnipsNLUEngine.from_path("models/" + str(self.lang))

			nlu_engine = snips_engines[self.lang]
			self.nlu_parsing = nlu_engine.parse(self.text)
			if(self.probability >= float(self.nlu_parsing["intent"]["probability"])):
				neural = self.nn(str(self.lang),self.text)
				if(neural == False and neural != None):
					self.nlu_parsing["intent"]["probability"] = float(0)
				elif(neural == True and neural != None):
					self.nlu_parsing["intent"]["probability"] = float(0.9)
		except Exception as e:
			print(e)
			# use lang detect from translator, try to load nlu again
			self.translate(src = True)

			try:
				if(self.lang in snips_engines.keys()):
					nlu_engine = snips_engines[self.lang]
				else:
					snips_engines[self.lang] = SnipsNLUEngine.from_path("models/" + str(self.lang))

				nlu_engine = snips_engines[self.lang]
				self.nlu_parsing = nlu_engine.parse(self.text)
				if(self.probability >= float(self.nlu_parsing["intent"]["probability"])):
					neural = self.nn(str(self.lang), self.text)
					if(neural == False and neural != None):
						self.nlu_parsing["intent"]["probability"] = float(0)
					elif(neural == True and neural != None):
						self.nlu_parsing["intent"]["probability"] = float(0.9)
			except Exception as e:
				print(e)

				# load default nlu, for case that langdetect failed
				if(self.mainlang in snips_engines.keys()):
					nlu_engine = snips_engines[self.mainlang]
				else:
					print("*** " + self.lang + " ***")
					snips_engines[self.lang] = SnipsNLUEngine.from_path("models/" + self.mainlang)

				nlu_engine = snips_engines[self.mainlang]

				self.nlu_parsing = nlu_engine.parse(self.translated)

				if(self.probability >= float(self.nlu_parsing["intent"]["probability"])):
					neural = self.nn(str(self.lang), self.translated)
					if(neural == False and neural != None):
						self.nlu_parsing["intent"]["probability"] = float(0)
					elif(neural == True and neural != None):
						self.nlu_parsing["intent"]["probability"] = float(0.9)


		self.nlu_parsing["lang"] = self.lang

		try:
			slots = self.nlu_parsing["slots"]
			for x in slots:
				self.nlu_parsing[x["slotName"]] = x["value"]["value"]


		except Exception as e:
			print(e)


		print(self.nlu_parsing)


	# second nlu for main language
	def nlu2(self):
		global snips_engines
		print("Second nlu")
		if(self.mainlang in snips_engines.keys()):
			nlu_engine = snips_engines[self.mainlang]
		else:
			snips_engines[self.lang] = SnipsNLUEngine.from_path("models/" + self.mainlang)

		nlu_engine = snips_engines[self.mainlang]
		self.translate()
		self.nlu_parsing = nlu_engine.parse(self.translated)
		self.nlu_parsing["lang"] = self.lang

		try:
			slots = self.nlu_parsing["slots"]
			for x in slots:
				self.nlu_parsing[x["slotName"]] = x["value"]["value"]


		except Exception as e:
			print(e)


		if(self.probability >= float(self.nlu_parsing["intent"]["probability"])):
			neural = self.nn(str(self.lang), self.translated)
			if(neural == False and neural != None):
				self.nlu_parsing["intent"]["probability"] = float(0)
			elif(neural == True and neural != None):
				self.nlu_parsing["intent"]["probability"] = float(0.9)

		print(self.nlu_parsing)





app = Flask(__name__)

# route to check number of commits
# also used to restart flask app, using debug mode, automatically after git pull command
@app.route('/version', methods=['GET'])
def hello():
	return "145 commits"


# return url for question
@app.route('/url', methods=['GET'])
def searxUrl():
	try:
		url = callAPI(request.args["text"],request.args["lang"])
		return str({"url": url})
	except:
		return str({"url": "https://a-ware.io/wp-content/uploads/2020/02/LOGO.png"})



@app.route('/langdetect', methods=['GET'])
def detectlangapi():
	return json.dumps({"text": detect(request.args["text"])}, indent=2, ensure_ascii=False)


@app.route('/translations', methods=['GET'])
def translateapi():
	ttt = request.args["text"]
	ttt = ttt.split("\n")
	translated = ""
	src = None
	for x in ttt:
		translates, src = translate(ttt, target = request.args["target"], src = True)
		translated += translates + "\n"
	return json.dumps({"text": translated, "source": src}, indent=2, ensure_ascii=False)




# main route
@app.route('/', methods=['POST', 'GET'])
def foo():
	"""
	important variables:
	text: contains the text need to be processed
	lang: is the language code from original language
	"""
	configData = None

	data = None
	nlu_engine = None
	translated = None
	# get all parameters, send by user
	if request.method == 'POST':
		try:
			data = str(request.json).replace("'","\"")
			#print(data)
		except Exception as e:
			print(e)

	if request.method == 'GET':
		if(configData != None):
			data = configData
		else:
			data = {}

		data.update(request.args)

		try:
			text = data["text"]
		except:
			return "No text given"



	try:
		if(data["text"] == None or len(data["text"]) < 2):
			return "No text given"

	except:
		return "No text given"



	# fit all data into main class
	helper = serverHelpers(data)

	# run first nlu
	helper.nlu()

	nlu_results = json.loads(json.dumps(helper.nlu_parsing, indent=2))
	skill = nlu_results["intent"]["intentName"]
	probability = float(nlu_results["intent"]["probability"])

	# if no skill exists or probability to low, check if google answer box exists
	if(skill == None or probability <= helper.probability):
		answer,answer_url = google(helper.text,json.dumps(helper.data, indent=2, ensure_ascii=False), language = helper.nlu_parsing["lang"])
		if(answer != False and answer != "False"):
			answer = str(answer).encode().decode()
			print(answer)
			if(answer_url != None):
					helper.nlu_parsing["answer_url"] = answer_url
			helper.nlu_parsing["speak"] = helper.parseAnswer(answer)
			helper.nlu_parsing["skill_category"] = "fallback"
			helper.nlu_parsing["answer_type"] = helper.answer_type
			if(len(helper.nlu_parsing["speak"]) > 1):
				results = json.dumps(helper.nlu_parsing, indent=2, ensure_ascii=False)
				helper.info("Fallback succesfull")
				return results

		# run second nlu if no answer box exists
		helper.nlu2()

		nlu_results = json.loads(json.dumps(helper.nlu_parsing, indent=2))
		skill = nlu_results["intent"]["intentName"]
		probability = float(nlu_results["intent"]["probability"])

		# run fallback if no skill exists again
		if(skill == None or probability <= helper.probability):

			return helper.callFallback()


	# run skills logic
	skill = skill.split("_")
	skill = skill[0]
	category = skill
	helper.nlu_parsing["skill_category"] = category

	skill = "skills." + skill + ".skill"
	mod=importlib.import_module(skill)
	results = ""

	try:
		url = None
		speak = mod.beginn(json.dumps(helper.data, indent=2, ensure_ascii=False), json.dumps(helper.nlu_parsing, indent=2).replace(category + "_", ""))
		# check the settings, the skill returns
		try:
			if(len(speak) == 2 and isinstance(speak, tuple)):
				url = speak[1]
				speak = speak[0]
			elif(len(speak) == 3 and isinstance(speak, tuple)):
				helper.answer_type = speak[2]
				url = speak[1]
				speak = speak[0]
			elif(len(speak) == 4 and isinstance(speak, tuple)):
				helper.useTranslator = speak[3]
				helper.answer_type = speak[2]
				url = speak[1]
				speak = speak[0]
			elif(len(speak) == 5 and isinstance(speak, tuple)):
				print(speak[4])
				helper.nlu_parsing.update(speak[4])
				helper.useTranslator = speak[3]
				helper.answer_type = speak[2]
				url = speak[1]
				speak = speak[0]

		except:
			pass

		info(skill)

		# if skill got no answer, check if google answer box exists
		if(speak == False):

			answer,answer_url = google(helper.text,json.dumps(helper.data, indent=2, ensure_ascii=False), language = helper.nlu_parsing["lang"])
			if(answer != False and answer != "False"):
				answer = str(answer).encode().decode()
				print(answer)
				if(answer_url != None):
						helper.nlu_parsing["answer_url"] = answer_url
				helper.nlu_parsing["speak"] = helper.parseAnswer(answer)
				helper.nlu_parsing["skill_category"] = "fallback"
				helper.nlu_parsing["answer_type"] = helper.answer_type

				if(len(helper.nlu_parsing["speak"]) > 1):
					results = json.dumps(helper.nlu_parsing, indent=2, ensure_ascii=False)
					helper.info("Fallback succesfull")
					return results


			try:
				return helper.callFallback()

			except Exception as e:
				print(e)


			results = "An error occured"

		else:
			# format results json and translate if necessary
			if(helper.useTranslator == True):
				try:
					helper.translate(text = speak,target = helper.lang)
					helper.nlu_parsing["speak"] = helper.parseAnswer(helper.translated)
				except:
					#speak = translate(speak,lang)
					helper.nlu_parsing["speak"] = helper.parseAnswer(speak)
			else:
				helper.nlu_parsing["speak"] = helper.parseAnswer(speak)

			if(url == None):
				helper.nlu_parsing["answer_url"] = "ask_search_engine"
			else:
				helper.nlu_parsing["answer_url"] = url

			helper.nlu_parsing["answer_type"] = helper.answer_type

			results = json.dumps(helper.nlu_parsing, indent=2, ensure_ascii=False)


	except Exception as e:
		print(e)
		results = "An error occured"
	return(results)

# run server on given port
if __name__ == "__main__":
	app.secret_key = os.urandom(12)
	app.run(host='127.0.0.1', port=args["port"], debug=True)
